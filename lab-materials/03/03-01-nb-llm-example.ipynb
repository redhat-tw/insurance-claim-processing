{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ad2cc4e-31ec-4648-b0fe-6632f2bdbc36",
   "metadata": {},
   "source": [
    "## 以程式方式使用 LLM\n",
    "\n",
    "在 Generative AI 興起的這個年代，許多人肯定與 ChatGPT 這種大型語言模型(LLM, Large Language Mode)互動過，且一般是透過 UI 或應用程式來完成。\n",
    "\n",
    "這個範例中，將使用 Python 透過其 API 直接連接與查詢已部署的 LLM，而本次 Lab 採用 **Mistral-7B Instruct v2**。(https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2) 模型，這是一個開源模型(Apache 2.0 license)，雖然比其他商業或開源模型輕量，但卻有很好的功能。\n",
    "\n",
    "P.S. 此模型預設已經部署到 Lab 叢集上，雖然是輕量的模型，但依然需要具備 24GB RAM 的 GPU 才能執行。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4e2b81-0e10-4390-a7b8-5ddfda53a3e3",
   "metadata": {},
   "source": [
    "### Requirements and Imports\n",
    "\n",
    "如果有依照前面流程啟用 Workbench Image 的話，預設都會包含所需 Libraries，因此可以直接 Import 使用。但如果未使用正確 Image 的話，則需要執行第一行 `pip install ...` 來安裝相依套件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61c595d-967e-47de-a598-02b5d1ccec85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install --no-cache-dir --no-dependencies --disable-pip-version-check -r requirements.txt # Uncomment only if you have not selected the right workbench image\n",
    "\n",
    "import json\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import HuggingFaceTextGenInference\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c428fbad-2345-4536-b687-72416d6b9b15",
   "metadata": {},
   "source": [
    "### Langchain\n",
    "\n",
    "LangChain 是開放原始碼的架構，可根據大型語言模型 (LLM) 打造應用程式。LLM 是針對大量資料預先訓練的大型深度學習模型，可以對使用者查詢生成回應，例如回答問題或從文本提示中建立影像。LangChain 提供工具和抽象化，藉以改善模型產生之資訊的客製化程度、準確度和關聯性。例如，開發人員可使用 LangChain 元件建立新提示鏈，或客製化現有的範本。LangChain 還包括其他元件，讓 LLM 無須重新訓練即可取得新資料。\n",
    "\n",
    "下面將建立一個 **llm** 實例，該實例由可以查詢 LLM API 的位置以及將應用於模型的一些參數來定義。e.g. `max_new_tokens` 定義模型最多使用 512 tokens(單字或單字的一部分)進行回答; 而 `temperature` 則表示模型回答接近事實(數值越低越接近)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f95a70-89fb-4e21-a51c-24e862b7953e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# LLM Inference Server URL\n",
    "inference_server_url = \"http://llm.ic-shared-llm.svc.cluster.local:3000/\"\n",
    "\n",
    "# LLM definition\n",
    "llm = HuggingFaceTextGenInference(\n",
    "    inference_server_url=inference_server_url,\n",
    "    max_new_tokens=512,\n",
    "    top_k=10,\n",
    "    top_p=0.95,\n",
    "    typical_p=0.95,\n",
    "    temperature=0.01,\n",
    "    repetition_penalty=1.03,\n",
    "    streaming=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b950bc-4d73-49e5-a35b-083a784edd50",
   "metadata": {},
   "source": [
    "接著還需要建立一個 **template** 來應用於將發送到模型的每個請求(即 \"Prompt\")。\n",
    "\n",
    "基於此，在查詢模型時，可以透過以下只是來提供模型給出正確的回答，例如以下情境:\n",
    "\n",
    "* 你(模型)是一位樂於助人、受人尊敬且誠實的助手。 在確保安全的同時，始終盡可能提供協助\n",
    "* 你(模型)的答案不應包含任何有害、不道德、種族主義、性別歧視、有毒、危險或非法內容。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bb7517-faa2-43ed-a95d-835de975f916",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "template=\"\"\"<s>[INST]<<SYS>>\n",
    "You are a helpful, respectful and honest assistant. Always be as helpful as possible, while being safe.\n",
    "You will be asked a question, to which you must give an answer.\n",
    "Your answer should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content.\n",
    "Please ensure that your responses are socially unbiased and positive in nature.\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\n",
    "If you don't know the answer to a question, answer \"I don't know\".\n",
    "<</SYS>>\n",
    "\n",
    "### QUESTION:\n",
    "{input}\n",
    "\n",
    "### ANSWER:\n",
    "[/INST]\n",
    "\"\"\"\n",
    "PROMPT = PromptTemplate(input_variables=[\"input\"], template=template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbe2119-2128-4432-aed1-126e9c8c034f",
   "metadata": {},
   "source": [
    "Langchain 現在允許將這些元素「縫合」在一起，並建立一個用來查詢模型的 **conversation** 物件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6d9f32-d4ae-4c2f-b513-d520413d2cc8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "conversation = LLMChain(llm=llm,\n",
    "                        prompt=PROMPT,\n",
    "                        verbose=False\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849fbd67-220c-4a02-8e4e-7e0d1aa91588",
   "metadata": {},
   "source": [
    "現在可以帶入查詢資訊來查看結果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca714bca-7cec-4afc-b275-fa389c05a993",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"什麼是 AI?\"\n",
    "\n",
    "conversation.predict(input=query); # \";\" at the end of the line hides final output (repetion of the streamed answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0089476-bba0-4093-8be8-1469780afaba",
   "metadata": {},
   "source": [
    "後續將在第 3.5 節回到此 Notebook 進行一些練習，因此可以暫時將其開著。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
