{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ad2cc4e-31ec-4648-b0fe-6632f2bdbc36",
   "metadata": {},
   "source": [
    "## 使用 LLM 進行文本摘要\n",
    "\n",
    "由於 LLM「理解」一種語言，因此它適合拿來做翻譯或摘要等等任務。\n",
    "\n",
    "而本 Notebook 中，將使用 LLM 來總結一些文本內容，特別是索賠範例內容。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4e2b81-0e10-4390-a7b8-5ddfda53a3e3",
   "metadata": {},
   "source": [
    "### Requirements and Imports\n",
    "\n",
    "如果有依照前面流程啟用 Workbench Image 的話，預設都會包含所需 Libraries，因此可以直接 Import 使用。但如果未使用正確 Image 的話，則需要執行第一行 `pip install ...` 來安裝相依套件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61c595d-967e-47de-a598-02b5d1ccec85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install --no-cache-dir --no-dependencies --disable-pip-version-check -r requirements.txt # Uncomment only if you have not selected the right workbench image\n",
    "\n",
    "import json\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import HuggingFaceTextGenInference\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c428fbad-2345-4536-b687-72416d6b9b15",
   "metadata": {},
   "source": [
    "### Langchain pipeline\n",
    "\n",
    "透過 Langchain 定義 summarization pipeline。並使用已經部署好的 **Mistral-7B Instruct v2** 來進行內容摘要產生。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f95a70-89fb-4e21-a51c-24e862b7953e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# LLM Inference Server URL\n",
    "inference_server_url = \"http://llm.ic-shared-llm.svc.cluster.local:3000/\"\n",
    "\n",
    "# LLM definition\n",
    "llm = HuggingFaceTextGenInference(\n",
    "    inference_server_url=inference_server_url,\n",
    "    max_new_tokens=512,\n",
    "    top_k=10,\n",
    "    top_p=0.95,\n",
    "    typical_p=0.95,\n",
    "    temperature=0.01,\n",
    "    repetition_penalty=1.03,\n",
    "    streaming=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b950bc-4d73-49e5-a35b-083a784edd50",
   "metadata": {},
   "source": [
    "建立一個 **template** 讓 LLM 模型針對特定內容摘要任務進行格式化。例如下面 Template 最後一段定義:\n",
    "\n",
    "`我會給你(模型)一篇文字，你(模型)必須盡可能地總結內容給我`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bb7517-faa2-43ed-a95d-835de975f916",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "template=\"\"\"<s>[INST]\n",
    "You are a helpful, respectful and honest assistant.\n",
    "Always assist with care, respect, and truth. Respond with utmost utility yet securely.\n",
    "Avoid harmful, unethical, prejudiced, or negative content. Ensure replies promote fairness and positivity.\n",
    "I will give you a text that you must summarize as best as you can.\n",
    "\n",
    "### TEXT:\n",
    "{input}\n",
    "\n",
    "### SUMMARY:\n",
    "[/INST]\n",
    "\"\"\"\n",
    "PROMPT = PromptTemplate(input_variables=[\"input\"], template=template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbe2119-2128-4432-aed1-126e9c8c034f",
   "metadata": {},
   "source": [
    "Langchain 現在允許將這些元素「縫合」在一起，並建立一個用來查詢模型的 **conversation** 物件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6d9f32-d4ae-4c2f-b513-d520413d2cc8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "conversation = LLMChain(llm=llm,\n",
    "                        prompt=PROMPT,\n",
    "                        verbose=False\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849fbd67-220c-4a02-8e4e-7e0d1aa91588",
   "metadata": {},
   "source": [
    "現在已經準備完成查詢模型了！\n",
    "\n",
    "此時可以從 `claims` 目錄中，使用一些一些 JSON 格式的範例內容來驗證。下面程式將讀取檔案內容，然後讓 LLM 幫我們產生摘要內容。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca714bca-7cec-4afc-b275-fa389c05a993",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read the claims and populate a dictionary\n",
    "claims_path = 'claims'\n",
    "onlyfiles = [f for f in listdir(claims_path) if isfile(join(claims_path, f))]\n",
    "\n",
    "claims = {}\n",
    "\n",
    "for filename in onlyfiles:\n",
    "    # Opening JSON file\n",
    "    with open(os.path.join(claims_path, filename), 'r') as file:\n",
    "        data = json.load(file)\n",
    "    claims[filename] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac009d5-d558-4258-9735-4fb0de46c309",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for filename in onlyfiles:\n",
    "    print(f\"***************************\")\n",
    "    print(f\"* Claim: {filename}\")\n",
    "    print(f\"***************************\")\n",
    "    print(\"Original content:\")\n",
    "    print(\"-----------------\")\n",
    "    print(f\"Subject: {claims[filename]['subject']}\\nContent:\\n{claims[filename]['content']}\\n\\n\")\n",
    "    print('Summary:')\n",
    "    print(\"--------\")\n",
    "    summary_input = f\"Subject: {claims[filename]['subject']}\\nContent:\\n{claims[filename]['content']}\"\n",
    "    conversation.predict(input=summary_input);\n",
    "    print(\"\\n\\n                          ----====----\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
