{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ad2cc4e-31ec-4648-b0fe-6632f2bdbc36",
   "metadata": {},
   "source": [
    "## 比較適用於不同模型差異\n",
    "\n",
    "在本 Notebook 中，將使用另一個模型 `Flan-T5-Small` 與 `Mistral-7B` 並行比較。\n",
    "\n",
    "`Flan-T5-Small` 好處是更加輕量，無須 GPU 即可執行，且只要 1GB RAM。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4e2b81-0e10-4390-a7b8-5ddfda53a3e3",
   "metadata": {},
   "source": [
    "### Requirements and Imports\n",
    "\n",
    "如果有依照前面流程啟用 Workbench Image 的話，預設都會包含所需 Libraries，因此可以直接 Import 使用。但如果未使用正確 Image 的話，則需要執行第一行 `pip install ...` 來安裝相依套件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61c595d-967e-47de-a598-02b5d1ccec85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install --no-cache-dir --no-dependencies --disable-pip-version-check -r requirements.txt # Uncomment only if you have not selected the right workbench image\n",
    "\n",
    "import json\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import HuggingFaceTextGenInference\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c428fbad-2345-4536-b687-72416d6b9b15",
   "metadata": {},
   "source": [
    "### Langchain pipeline\n",
    "\n",
    "這邊將定義兩個不同的 LLM Endpoints，以及兩個不同的 Pipelines。\n",
    "\n",
    "P.S. 以下兩個模型都事前部署在 OpenShift 叢集中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f95a70-89fb-4e21-a51c-24e862b7953e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Main LLM Inference Server URL\n",
    "inference_server_url = \"http://llm.ic-shared-llm.svc.cluster.local:3000/\"\n",
    "\n",
    "# LLM definition\n",
    "llm = HuggingFaceTextGenInference(\n",
    "    inference_server_url=inference_server_url,\n",
    "    max_new_tokens=512,\n",
    "    top_k=10,\n",
    "    top_p=0.95,\n",
    "    typical_p=0.95,\n",
    "    temperature=0.01,\n",
    "    repetition_penalty=1.03,\n",
    "    streaming=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782046b7-f0a4-487c-86fc-3131e668c6c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Flan-T5-Small LLM Inference Server URL\n",
    "inference_server_url_flan_t5 = \"http://llm-flant5.ic-shared-llm.svc.cluster.local:3000/\"\n",
    "\n",
    "# LLM definition\n",
    "llm_flant5 = HuggingFaceTextGenInference(\n",
    "    inference_server_url=inference_server_url_flan_t5,\n",
    "    max_new_tokens=96,\n",
    "    top_k=10,\n",
    "    top_p=0.95,\n",
    "    typical_p=0.95,\n",
    "    temperature=0.01,\n",
    "    repetition_penalty=1.03,\n",
    "    streaming=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b950bc-4d73-49e5-a35b-083a784edd50",
   "metadata": {},
   "source": [
    "兩個 LLM 都使用同樣的 `template` 讓 LLM 模型針對特定要求回答。例如下面 Template 最後一段定義:\n",
    "\n",
    "`我會給你(模型)一條短信，然後問一個關於它的問題。對這個問題給予盡可能準確和簡潔的答案`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bb7517-faa2-43ed-a95d-835de975f916",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "template=\"\"\"<s>[INST]\n",
    "You are a helpful, respectful and honest assistant.\n",
    "Always assist with care, respect, and truth. Respond with utmost utility yet securely.\n",
    "Avoid harmful, unethical, prejudiced, or negative content. Ensure replies promote fairness and positivity.\n",
    "I will give you a text, then ask a question about it. Give a precise and as concise as possible answer to this question.\n",
    "\n",
    "### TEXT:\n",
    "{text}\n",
    "\n",
    "### QUESTION:\n",
    "{query}\n",
    "\n",
    "### ANSWER:\n",
    "[/INST]\n",
    "\"\"\"\n",
    "PROMPT = PromptTemplate(input_variables=[\"input\"], template=template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbe2119-2128-4432-aed1-126e9c8c034f",
   "metadata": {},
   "source": [
    "同前幾個範例，建立兩個用來查詢模型的 **conversation** 物件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6d9f32-d4ae-4c2f-b513-d520413d2cc8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "conversation = LLMChain(llm=llm,\n",
    "                        prompt=PROMPT,\n",
    "                        verbose=False\n",
    "                        )\n",
    "                        \n",
    "conversation_flant5 = LLMChain(llm=llm_flant5,\n",
    "                        prompt=PROMPT,\n",
    "                        verbose=False\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849fbd67-220c-4a02-8e4e-7e0d1aa91588",
   "metadata": {},
   "source": [
    "現在已經準備完成查詢模型了！\n",
    "\n",
    "在此範例中，將查詢一個範例檔案來看看會發生什麼事情。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac009d5-d558-4258-9735-4fb0de46c309",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filename = 'claims/claim1.json'\n",
    "\n",
    "# Opening JSON file\n",
    "claims = {}\n",
    "with open(filename, 'r') as file:\n",
    "    data = json.load(file)\n",
    "claims[filename] = data\n",
    "\n",
    "# Analyze the claim\n",
    "print(f\"***************************\")\n",
    "print(f\"* Claim: {filename}\")\n",
    "print(f\"***************************\")\n",
    "print(\"Original content:\")\n",
    "print(\"-----------------\")\n",
    "print(f\"Subject: {claims[filename]['subject']}\\nContent:\\n{claims[filename]['content']}\\n\\n\")\n",
    "print('Analysis with Mistral-7B:')\n",
    "print(\"--------\")\n",
    "text_input = f\"Subject: {claims[filename]['subject']}\\nContent:\\n{claims[filename]['content']}\"\n",
    "sentiment_query = \"What is the sentiment of the person sending this claim?\"\n",
    "location_query = \"Where does the event the claim is related to happen?\"\n",
    "time_query = \"When does the event the claim is related to happen? If possible, specify the date and the time.\"\n",
    "print(f\"- Sentiment: \")\n",
    "conversation.predict(text=text_input, query=sentiment_query);\n",
    "print(\"\\n- Location: \")\n",
    "conversation.predict(text=text_input, query=location_query);\n",
    "print(\"\\n- Time: \")\n",
    "conversation.predict(text=text_input, query=time_query);\n",
    "print(\"\\n\\n                          ----====----\\n\")\n",
    "print('Analysis with Flan-T5-Small:')\n",
    "print(\"--------\")\n",
    "text_input = f\"Subject: {claims[filename]['subject']}\\nContent:\\n{claims[filename]['content']}\"\n",
    "sentiment_query = \"What is the sentiment of the person sending this claim?\"\n",
    "location_query = \"Where does the event the claim is related to happen?\"\n",
    "time_query = \"When does the event the claim is related to happen? If possible, specify the date and the time.\"\n",
    "print(f\"- Sentiment: \")\n",
    "conversation_flant5.predict(text=text_input, query=sentiment_query);\n",
    "print(\"\\n- Location: \")\n",
    "conversation_flant5.predict(text=text_input, query=location_query);\n",
    "print(\"\\n- Time: \")\n",
    "conversation_flant5.predict(text=text_input, query=time_query);\n",
    "print(\"\\n\\n                          ----====----\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e28a5b0-6c93-42ba-84dd-42e17746d11d",
   "metadata": {},
   "source": [
    "會看到 `Flan-T5-Small` 效率比較好，因為該模型是個只有 8000 萬參數的 LLM。雖然查詢效率高，但跟`Mistral-7B` 比較的話，會發現結果準確性相差甚遠。因為 `Mistral-7B` 具有 70 億參數。\n",
    "\n",
    "在使用 LLM 時，需要根據`效能`、`準確性`、`所需資源`與`相關成本`之間找到適當平衡。因此多個模型比較非常重要，開發者或使用者可以透過`合理性檢驗(Sanity checks)`來確保資料變化或模型發展，始終保持預期內。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
